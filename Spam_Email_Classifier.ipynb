{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOZ7ClcLj9oeC3ABg0UZR6W"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Js8BchfzT6C5"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas numpy scikit-learn nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5RTt78XgUmpB",
        "outputId": "1d8389c5-27d5-44d4-98d2-ce226c7ff412"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://spamassassin.apache.org/old/publiccorpus/20030228_spam.tar.bz2\n",
        "!wget https://spamassassin.apache.org/old/publiccorpus/20030228_easy_ham.tar.bz2\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RAvkQmNMVDvD",
        "outputId": "176217df-48cb-43cf-8e29-12eac8dbc06d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-02-20 17:07:41--  https://spamassassin.apache.org/old/publiccorpus/20030228_spam.tar.bz2\n",
            "Resolving spamassassin.apache.org (spamassassin.apache.org)... 151.101.2.132, 2a04:4e42::644\n",
            "Connecting to spamassassin.apache.org (spamassassin.apache.org)|151.101.2.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1183768 (1.1M) [application/x-bzip2]\n",
            "Saving to: ‘20030228_spam.tar.bz2’\n",
            "\n",
            "20030228_spam.tar.b 100%[===================>]   1.13M  --.-KB/s    in 0.05s   \n",
            "\n",
            "2025-02-20 17:07:42 (22.3 MB/s) - ‘20030228_spam.tar.bz2’ saved [1183768/1183768]\n",
            "\n",
            "--2025-02-20 17:07:42--  https://spamassassin.apache.org/old/publiccorpus/20030228_easy_ham.tar.bz2\n",
            "Resolving spamassassin.apache.org (spamassassin.apache.org)... 151.101.2.132, 2a04:4e42::644\n",
            "Connecting to spamassassin.apache.org (spamassassin.apache.org)|151.101.2.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1612216 (1.5M) [application/x-bzip2]\n",
            "Saving to: ‘20030228_easy_ham.tar.bz2’\n",
            "\n",
            "20030228_easy_ham.t 100%[===================>]   1.54M  --.-KB/s    in 0.04s   \n",
            "\n",
            "2025-02-20 17:07:43 (34.3 MB/s) - ‘20030228_easy_ham.tar.bz2’ saved [1612216/1612216]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tarfile\n",
        "\n",
        "def extract_tar(file_name):\n",
        "    with tarfile.open(file_name, \"r:bz2\") as tar:\n",
        "        tar.extractall()\n",
        "\n",
        "extract_tar(\"20030228_spam.tar.bz2\")\n",
        "extract_tar(\"20030228_easy_ham.tar.bz2\")\n"
      ],
      "metadata": {
        "id": "8Zjs92NpVLw7"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "import nltk\n",
        "import re\n",
        "\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Load spam and ham emails\n",
        "spam_dir = \"spam\"\n",
        "ham_dir = \"easy_ham\"\n",
        "\n",
        "def load_emails(directory, label):\n",
        "    emails = []\n",
        "    for filename in os.listdir(directory):\n",
        "        with open(os.path.join(directory, filename), \"r\", encoding=\"latin-1\") as f:\n",
        "            emails.append((f.read(), label))\n",
        "    return emails\n",
        "\n",
        "spam_emails = load_emails(spam_dir, 1)\n",
        "ham_emails = load_emails(ham_dir, 0)\n",
        "\n",
        "# Create DataFrame\n",
        "df = pd.DataFrame(spam_emails + ham_emails, columns=[\"text\", \"label\"])\n",
        "\n",
        "# Preprocess text\n",
        "def clean_text(text):\n",
        "    text = re.sub(r'\\W+', ' ', text)  # Remove special characters\n",
        "    text = text.lower()  # Convert to lowercase\n",
        "    text = ' '.join([word for word in text.split() if word not in stopwords.words('english')])  # Remove stopwords\n",
        "    return text\n",
        "\n",
        "df[\"text\"] = df[\"text\"].apply(clean_text)\n",
        "\n",
        "# Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(df[\"text\"], df[\"label\"], test_size=0.2, random_state=42)\n",
        "\n",
        "# Convert text to numerical features\n",
        "vectorizer = TfidfVectorizer()\n",
        "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
        "X_test_tfidf = vectorizer.transform(X_test)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "un6biD7mVUpE",
        "outputId": "ea183b5e-a344-48a2-8e69-2d7d75deadc7"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train model\n",
        "model = MultinomialNB()\n",
        "model.fit(X_train_tfidf, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test_tfidf)\n",
        "\n",
        "# Evaluate model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy: {accuracy:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1-YnToqTWId3",
        "outputId": "d85e17b6-f4cb-48d2-c8b4-0591e9522480"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 0.86\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_spam(email_text):\n",
        "    email_tfidf = vectorizer.transform([email_text])\n",
        "    prediction = model.predict(email_tfidf)[0]\n",
        "    return \"Spam\" if prediction == 1 else \"Not Spam\"\n",
        "\n",
        "# Example\n",
        "email = \"Congratulations! You've won a free iPhone! Click here to claim.\"\n",
        "print(predict_spam(email))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2BwOss31WUYW",
        "outputId": "cacc19ab-329e-4d45-9ae1-5dfa513d0ede"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Not Spam\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "# Save model and vectorizer\n",
        "pickle.dump(model, open(\"spam_classifier.pkl\", \"wb\"))\n",
        "pickle.dump(vectorizer, open(\"vectorizer.pkl\", \"wb\"))\n"
      ],
      "metadata": {
        "id": "YKHGSvWOWgSK"
      },
      "execution_count": 7,
      "outputs": []
    }
  ]
}